# Cell 1: å®‰è£…ä¾èµ–åº“
print("â³ æ­£åœ¨å®‰è£…ä¾èµ–åº“ï¼Œè¯·ç¨ç­‰...")
# å®‰è£… Hugging Face ç›¸å…³åº“åŠéŸ³é¢‘å¤„ç†åº“
!pip install datasets transformers accelerate librosa evaluate torch soundfile==0.12.1 --quiet
!pip install --upgrade accelerate --quiet
print("âœ… çŽ¯å¢ƒå®‰è£…å®Œæˆï¼")
# Cell 2: åŠ è½½æ•°æ®ä¸ŽæŸ¥çœ‹æ ‡ç­¾
import warnings
from datasets import load_dataset, Audio, ClassLabel

warnings.filterwarnings("ignore")
print("â³ æ­£åœ¨åŠ è½½ç‹—ç‹—æƒ…ç»ªæ•°æ®é›† (cgeorgiaw/animal-sounds)...")

# 1. åŠ è½½ "dogs" å­é›† (åŒ…å« è­¦æˆ’ã€å­¤ç‹¬ã€çŽ©è€ ä¸‰ç§æƒ…ç»ª)
try:
    # ç§»é™¤äº† trust_remote_code=True å‚æ•°ï¼Œå› ä¸ºè¯¥å‚æ•°å·²ä¸è¢«æŽ¨èä½¿ç”¨ã€‚
    dataset = load_dataset("cgeorgiaw/animal-sounds", "dogs", split="train")
    # Debugging: æ‰“å°æ•°æ®é›†ç‰¹å¾ä»¥æ‰¾å‡ºæ­£ç¡®çš„æ ‡ç­¾åˆ—å
    print(f"DEBUG: Dataset features keys for 'dogs' subset: {dataset.features.keys()}")

    # FIX: æƒ…ç»ªæ ‡ç­¾åœ¨ 'context' åˆ—ä¸­ï¼Œéœ€è¦ä»Žè¿™é‡Œæå–å¹¶åˆ›å»º 'label' åˆ—
    unique_contexts = sorted(list(set(dataset["context"])))
    labels = unique_contexts

    print("âœ… æˆåŠŸåŠ è½½ 'dogs' å­é›†ã€‚")
except Exception as e:
    print(f"âš ï¸ æ— æ³•åŠ è½½ 'dogs' å­é›†æˆ–æå–æ ‡ç­¾: {e}")
    print("âŒ æ— æ³•ç»§ç»­ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æˆ–ç½‘ç»œè¿žæŽ¥ã€‚")
    raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œåœæ­¢ç¨‹åºã€‚

# 2. èŽ·å–æ ‡ç­¾ä¿¡æ¯
# è¿™æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œæˆ‘ä»¬è¦ç¡®è®¤ dataset é‡Œæœ‰å“ªäº›æƒ…ç»ª

print(f"ðŸ“Š æ•°æ®é›†åŒ…å«çš„æƒ…ç»ªæ ‡ç­¾: {labels}")
# é¢„æœŸè¾“å‡º: ['disturbance', 'isolation', 'play'] ç­‰ (çŽ°åœ¨åº”è¯¥èƒ½æ­£ç¡®è¾“å‡ºç‹—ç‹—æƒ…ç»ªæ ‡ç­¾äº†)

# 3. åˆ¶ä½œæ ‡ç­¾æ˜ å°„å­—å…¸ (è®© AI è¯»æ‡‚è¿™äº›è¯)
label2id = {label: str(i) for i, label in enumerate(labels)}
id2label = {str(i): label for i, label in enumerate(labels)}

# 4. å°† 'context' åˆ—æ˜ å°„åˆ°æ–°çš„ 'label' åˆ—ï¼Œå¹¶è¿›è¡Œç±»åž‹è½¬æ¢
def map_context_to_label(example):
    example['label'] = label2id[example['context']]
    return example

dataset = dataset.map(map_context_to_label)
# ç§»é™¤åŽŸå§‹çš„ 'context' åˆ—ï¼Œå¦‚æžœä¸å†éœ€è¦
dataset = dataset.remove_columns(['context'])
# è½¬æ¢ä¸º ClassLabel ç±»åž‹
dataset = dataset.cast_column("label", ClassLabel(names=labels))

# 5. åˆ’åˆ†è®­ç»ƒé›† (80%) å’Œæµ‹è¯•é›† (20%)
dataset = dataset.train_test_split(test_size=0.2, seed=42)

print(f"âœ… æ•°æ®å‡†å¤‡å°±ç»ªï¼")
print(f"   - è®­ç»ƒé›†æ ·æœ¬: {len(dataset['train'])}")
print(f"   - æµ‹è¯•é›†æ ·æœ¬: {len(dataset['test'])}")
# Cell 3.5: ä½¿ç”¨ SMOTE è¿‡é‡‡æ ·æŠ€æœ¯å¹³è¡¡è®­ç»ƒé›†
import numpy as np
from imblearn.over_sampling import SMOTE
from collections import Counter
from datasets import Dataset # å¯¼å…¥ Dataset ç”¨äºŽä»Ž NumPy æ•°ç»„åˆ›å»º Hugging Face Dataset

print("â³ æ­£åœ¨å¹³è¡¡è®­ç»ƒé›†ä¸­çš„æƒ…ç»ªç±»åˆ«...")

# 1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šæå–ç‰¹å¾å’Œæ ‡ç­¾
# ç¡®ä¿ 'input_values' è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œä»¥ä¾¿ SMOTE å¤„ç†
X_train = np.array(encoded_dataset['train']['input_values'])
y_train = np.array(encoded_dataset['train']['label'])

print(f"å¹³è¡¡å‰è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(y_train)}")

# 2. åº”ç”¨ SMOTE è¿›è¡Œè¿‡é‡‡æ ·
sm = SMOTE(random_state=42, k_neighbors=1) # k_neighbors=1 æ˜¯æœ€å°è®¾ç½®ï¼Œé€‚ç”¨äºŽæ ·æœ¬é‡è¾ƒå°‘çš„ç±»åˆ«
X_resampled, y_resampled = sm.fit_resample(X_train, y_train)

print(f"å¹³è¡¡åŽè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {Counter(y_resampled)}")

# 3. å°†å¹³è¡¡åŽçš„æ•°æ®è½¬æ¢å›ž Hugging Face Dataset æ ¼å¼
# éœ€è¦é‡æ–°æž„é€ ä¸€ä¸ª Dataset å¯¹è±¡
encoded_dataset_balanced_train = Dataset.from_dict({
    'input_values': X_resampled.tolist(), # SMOTE è¾“å‡ºçš„æ˜¯ NumPy æ•°ç»„ï¼Œéœ€è¦è½¬å›žåˆ—è¡¨
    'labels': y_resampled.tolist() # æ ‡ç­¾ä¹Ÿè¦è½¬å›žåˆ—è¡¨
})

# ç¡®ä¿æ–°æ•°æ®é›†åŒ…å«å…¶ä»–åŽŸå§‹åˆ—ï¼Œä¾‹å¦‚ attention_maskï¼Œè¿™åœ¨ Wav2Vec2 ä¸­å¾ˆé‡è¦
# ç”±äºŽ SMOTEåªå¤„ç† input_values å’Œ labelsï¼Œå…¶ä»–åˆ—å°†ä¸¢å¤±ã€‚æˆ‘ä»¬éœ€è¦é‡æ–°æ·»åŠ å®ƒä»¬ã€‚
# è¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºSMOTEä¼šç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œæ²¡æœ‰å¯¹åº”çš„åŽŸå§‹ attention_maskã€‚
# ç®€åŒ–çš„å¤„ç†æ–¹å¼æ˜¯å‡è®¾æ‰€æœ‰æ–°æ ·æœ¬éƒ½ä½¿ç”¨ä¸ŽåŽŸå§‹æ ·æœ¬ç›¸åŒçš„ attention_mask ç­–ç•¥ã€‚
# ä½†æ›´ç¨³å¦¥çš„æ˜¯ç›´æŽ¥åœ¨ Trainer ä¸­å¤„ç† batch æ•°æ®çš„ padding/attention_maskã€‚
# å¯¹äºŽ Wav2Vec2ï¼Œinput_values å·²ç»æ˜¯å›ºå®šé•¿åº¦ä¸”padding=True, truncation=Trueã€‚
# å› æ­¤ï¼Œå¯ä»¥å‡è®¾ attention_mask æ˜¯å…¨1ï¼ˆå¦‚æžœ max_length == å®žé™…é•¿åº¦ï¼‰æˆ–è€…åŸºäºŽ padding ç”Ÿæˆã€‚
# ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾ `input_values` çš„å½¢çŠ¶å’ŒåŽŸå§‹æ•°æ®ä¸€è‡´ï¼Œå¯ä»¥ä»ŽåŽŸå§‹æ•°æ®é›†ä¸­æå– `attention_mask`ã€‚

# æˆ‘ä»¬å¯ä»¥æž„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦ç‰¹å¾çš„æ–° Dataset
# è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒSMOTEåªä¼šå¯¹X_trainå’Œy_trainè¿›è¡Œæ“ä½œï¼Œå¹¶ä¸ä¼šç”Ÿæˆattention_mask
# æˆ‘ä»¬å¯ä»¥å…ˆåˆ›å»ºä¸€ä¸ªåªæœ‰ input_values å’Œ labels çš„ Datasetï¼Œç„¶åŽåœ¨ Trainer ä¸­å¤„ç† padding å’Œ attention_mask
# ä½†ä¸ºäº†ä¸ŽåŽŸå§‹ `encoded_dataset` ç»“æž„ä¸€è‡´ï¼Œæˆ‘ä»¬æœ€å¥½ä¹Ÿä¸º resampled data ç”Ÿæˆ attention_maskã€‚

# é‡æ–°åˆ›å»º balanced_train_datasetï¼Œå¹¶æ·»åŠ ä¸€ä¸ªé»˜è®¤çš„ attention_mask (å…¨1ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»å›ºå®šäº†é•¿åº¦)
# æ³¨æ„: ç†è®ºä¸Šï¼Œattention_maskåº”è¯¥ç”± feature_extractor æ ¹æ®å®žé™…æ•°æ®é•¿åº¦ç”Ÿæˆã€‚
# ä½†åœ¨è¿™é‡Œï¼ŒSMOTEç”Ÿæˆçš„æ–°æ•°æ®ï¼Œæˆ‘ä»¬æ²¡æœ‰åŽŸå§‹çš„éŸ³é¢‘é•¿åº¦ä¿¡æ¯ã€‚
# é‰´äºŽ `input_values` å·²ç»è¿‡ `feature_extractor` ç»Ÿä¸€é•¿åº¦å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ `attention_mask` éƒ½æ˜¯å…¨1ã€‚

attention_mask_resampled = np.ones(X_resampled.shape, dtype=int)

encoded_dataset['train'] = Dataset.from_dict({
    'input_values': X_resampled.tolist(),
    'attention_mask': attention_mask_resampled.tolist(),
    'labels': y_resampled.tolist()
})

print("âœ… è®­ç»ƒé›†æƒ…ç»ªç±»åˆ«å¹³è¡¡å®Œæˆï¼")
# Cell 3: ç‰¹å¾æå– (å½¢çŠ¶å¼ºåŠ›å¯¹é½ç‰ˆ)
import numpy as np
import soundfile as sf
import io
import librosa
from transformers import AutoFeatureExtractor
from datasets import ClassLabel # å¯¼å…¥ ClassLabel

print("â³ æ­£åœ¨å¤„ç†éŸ³é¢‘ç‰¹å¾...")

# 1. åŠ è½½ç‰¹å¾æå–å™¨
model_checkpoint = "facebook/wav2vec2-base"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)

# **FIX:** ç¡®ä¿ 'label' åˆ—å­˜åœ¨å¹¶å…·æœ‰æ­£ç¡®çš„ç±»åž‹
# æ•°æ®é›†é»˜è®¤å…·æœ‰ 'class' åˆ—ã€‚åœ¨è½¬æ¢å…¶ç±»åž‹ä¹‹å‰ï¼Œå°†å…¶é‡å‘½åä¸º 'label'ã€‚
# å¦‚æžœ 'class' åˆ—ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä»Ž 'name' åˆ—æŽ¨æ–­ 'label'ã€‚

if 'class' in dataset['train'].column_names:
    dataset = dataset.rename_column("class", "label")
    print("âœ… å·²å°† 'class' åˆ—é‡å‘½åä¸º 'label'ã€‚")
elif 'label' not in dataset['train'].column_names: # Only attempt to create if 'label' is also missing
    print("âš ï¸ æ•°æ®é›†ä¸­æœªæ‰¾åˆ° 'class' åˆ—ã€‚å°è¯•ä»Ž 'name' åˆ—åˆ›å»º 'label'ã€‚")
    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»Ž 'name' å­—æ®µä¸­æå–æ ‡ç­¾ä¿¡æ¯
    def assign_label_from_name(example):
        found_label = None
        for animal_label in labels: # 'labels' comes from Cell 2
            if animal_label in example['name'].lower(): # Check if animal name is in the example's 'name' field
                found_label = animal_label
                break
        if found_label:
            example['label'] = int(label2id[found_label])
        else:
            # Fallback for cases where no animal name is found in 'name'
            print(f"âš ï¸ Warning: Could not infer label for name: {example['name']}. Assigning to {labels[0]} as fallback.")
            example['label'] = int(label2id[labels[0]]) # Assign to the first label as a fallback
        return example
    dataset = dataset.map(assign_label_from_name)
    print("âœ… æˆåŠŸä»Ž 'name' åˆ—åˆ›å»º 'label' åˆ—ã€‚")
else:
    print("âœ… 'label' åˆ—å·²å­˜åœ¨ï¼Œæ— éœ€é¢å¤–å¤„ç†ã€‚")

# 2. å…³é—­è‡ªåŠ¨è§£ç å¹¶è½¬æ¢ 'label' åˆ—ç±»åž‹
# çŽ°åœ¨ 'label' åˆ—åº”è¯¥å·²ç»å­˜åœ¨äº†ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œç±»åž‹è½¬æ¢ã€‚
dataset = dataset.cast_column("label", ClassLabel(names=labels)) # ç¡®ä¿æ ‡ç­¾ç±»åž‹æ­£ç¡®
dataset = dataset.cast_column("audio", Audio(decode=False))

# 3. å®šä¹‰â€œå¼ºåŠ›å¯¹é½â€å¤„ç†å‡½æ•°
def preprocess_function(examples):
    audio_arrays = []
    target_sr = 16000
    target_length = 24000  # 1.5ç§’

    for audio_data in examples["audio"]:
        try:
            # A. è¯»å–éŸ³é¢‘
            if "bytes" in audio_data and audio_data["bytes"]:
                array, sr = sf.read(io.BytesIO(audio_data["bytes"]))
            elif "path" in audio_data and audio_data["path"]:
                array, sr = sf.read(audio_data["path"])
            else:
                array = np.zeros(target_length)
                sr = target_sr

            # B. ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶è½¬å•å£°é“ (Mono)
            # å¦‚æžœæ˜¯ç«‹ä½“å£° (N, 2)ï¼Œlibrosa æˆ–è€…æ˜¯ sf è¯»å–å‡ºæ¥å¯èƒ½æ˜¯äºŒç»´æ•°ç»„
            if len(array.shape) > 1:
                # å–å¹³å‡å€¼è½¬ä¸ºå•å£°é“ï¼Œæˆ–è€…ç›´æŽ¥å–ç¬¬ä¸€ä¸ªå£°é“
                array = np.mean(array, axis=1)

            # C. é‡é‡‡æ ·åˆ° 16000Hz
            if sr != target_sr:
                array = librosa.resample(array, orig_sr=sr, target_sr=target_sr)

            # D. ã€å…³é”®ä¿®å¤ã€‘ä¸¥æ ¼ç»Ÿä¸€é•¿åº¦ (Trim or Pad)
            current_len = len(array)
            if current_len > target_length:
                # å¤ªé•¿äº†ï¼Œåˆ‡æŽ‰
                array = array[:target_length]
            elif current_len < target_length:
                # å¤ªçŸ­äº†ï¼Œè¡¥é›¶
                padding = target_length - current_len
                array = np.pad(array, (0, padding), "constant")

            # åŒé‡ä¿é™©ï¼šç¡®ä¿ä¸€å®šæ˜¯ 24000 é•¿åº¦
            if len(array) != target_length:
                 array = np.resize(array, target_length)

            audio_arrays.append(array)

        except Exception as e:
            # é‡åˆ°ä»»ä½•åæ•°æ®ï¼Œå¡«å…¥å…¨0é™éŸ³ï¼Œä¿è¯ç¨‹åºä¸å´©
            print(f"âš ï¸ è·³è¿‡åæ•°æ®: {e}")
            audio_arrays.append(np.zeros(target_length))

    # E. è°ƒç”¨æå–å™¨
    # æ­¤æ—¶ audio_arrays é‡Œçš„æ¯ä¸€ä¸ªå…ƒç´ å½¢çŠ¶éƒ½æ˜¯ä¸¥æ ¼çš„ (24000,)
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=target_sr,
        max_length=target_length,
        truncation=True,
        padding=True
    )
    return inputs

# 4. æ‰¹é‡å¤„ç†
# æ­¤æ—¶åº”è¯¥èƒ½é¡ºç•…è·‘é€šäº†
print("å¼€å§‹æ‰¹é‡æå–ç‰¹å¾ (Batch Processing)...")
encoded_dataset = dataset.map(preprocess_function, batched=True, batch_size=4)
print("âœ… ç‰¹å¾æå–å®Œæˆï¼æ•°æ®å½¢çŠ¶å·²å®Œç¾Žå¯¹é½ã€‚")
# Cell 4: å¼€å§‹å¾®è°ƒè®­ç»ƒ
import evaluate
from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

print("â³ æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒ...")

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
model = AutoModelForAudioClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(labels),
    label2id=label2id,
    id2label=id2label,
)

# 2. å®šä¹‰è¯„ä¼°æ–¹æ³• (å‡†ç¡®çŽ‡)
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)

# 3. è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./dog_emotion_model",
    eval_strategy="epoch",  # æ¯ä¸ª epoch æµ‹ä¸€æ¬¡åˆ†
    save_strategy="epoch",  # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
    learning_rate=3e-5,     # å­¦ä¹ çŽ‡
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,     # å»ºè®®è·‘ 5 è½®ï¼Œè®©å®ƒå­¦é€ä¸€ç‚¹
    logging_steps=10,
    load_best_model_at_end=True, # è®­ç»ƒç»“æŸä¿ç•™æœ€å¥½çš„é‚£ä¸ª
    metric_for_best_model="accuracy"
)

# 4. åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics,
)

# 5. å¼€è·‘ï¼
print("ðŸš€ å¼€å§‹å¾®è°ƒ (Start Fine-tuning)...")
trainer.train()

# 6. ä¿å­˜æœ€ç»ˆæ¨¡åž‹
trainer.save_model("./final_emotion_model")
feature_extractor.save_pretrained("./final_emotion_model")
print("ðŸŽ‰ è®­ç»ƒå®Œæˆï¼æ¨¡åž‹å·²ä¿å­˜åˆ° ./final_emotion_model")
# ==============================================================================
# ðŸ¶ ç‹—ç‹—æƒ…ç»ªç¿»è¯‘å®˜ - æœ€ç»ˆä¿®å¤ç‰ˆ (ä¿®æ­£ç¿»è¯‘ç±»åˆ«ï¼Œå¹¶ç¡®ä¿è¯­éŸ³å®Œæ•´è¾“å‡º)
# ==============================================================================

# 1. å¿…è¦çš„åº“
import random
import librosa
import numpy as np
from transformers import pipeline
from IPython.display import Audio, display
from google.colab import files

print("ðŸ¶ æ­£åœ¨æ›´æ–°ç¿»è¯‘å‰§æœ¬...")

# åŠ è½½æ¨¡åž‹ (æŒ‡å‘ä½ åˆšæ‰è®­ç»ƒå¥½çš„æ–‡ä»¶å¤¹)
# ç¡®ä¿è¿™é‡Œè·¯å¾„æ˜¯å¯¹çš„
my_model_path = "./final_emotion_model"
classifier = pipeline("audio-classification", model=my_model_path)
tts = pipeline("text-to-speech", model="facebook/mms-tts-eng")

# --- ðŸ“ å…³é”®ä¿®å¤ï¼šæ›´æ–°ç¿»è¯‘å­—å…¸ï¼Œä½¿å…¶ä¸Žæ¨¡åž‹é¢„æµ‹çš„ç‹—ç‹—æƒ…ç»ªæ ‡ç­¾ä¸€è‡´ ---
translation_script = {
    # 1. æ”»å‡» (Aggression) - å¯¹åº” 'aggression'
    'aggression': [
        "ðŸ˜¡ ç¿»è¯‘: 'ç¦»æˆ‘è¿œç‚¹ï¼æˆ‘åœ¨ç”Ÿæ°”ï¼(Get away! I'm angry!)'",
        "ðŸ˜¡ ç¿»è¯‘: 'è¿™æ˜¯æˆ‘çš„åœ°ç›˜ï¼ä¸å‡†é è¿‘ï¼(My territory! Stay back!)'",
        "ðŸ˜¡ ç¿»è¯‘: 'åˆ«æƒ¹æˆ‘ï¼Œå°å¿ƒæˆ‘å’¬ä½ ï¼(Don't provoke me!)'"
    ],
    # 2. è”ç»œ/å‘¼å”¤ (Contact) - å¯¹åº” 'contact'
    'contact': [
        "ðŸ‘‹ ç¿»è¯‘: 'å“ˆå–½ï¼Ÿæœ‰äººåœ¨å—ï¼Ÿ(Hello? Is anyone here?)'",
        "ðŸ‘‹ ç¿»è¯‘: 'æˆ‘åœ¨è¿™å„¿ï¼ä½ ä»¬åœ¨å“ªå‘¢ï¼Ÿ(I'm here! Where are you?)'",
        "ðŸ‘‹ ç¿»è¯‘: 'ä¸»äººï¼Œçœ‹æˆ‘ä¸€çœ¼å˜›ï¼(Master, look at me!)'"
    ],
    # 3. çŽ©è€ (Play) - å¯¹åº” 'play'
    'play': [
        "ðŸ˜„ ç¿»è¯‘: 'å¿«æŠŠçƒæ‰”è¿‡æ¥ï¼æˆ‘ä»¬æ¥çŽ©å‘€ï¼(Throw the ball! Let's play!)'",
        "ðŸ˜„ ç¿»è¯‘: 'æ¥è¿½æˆ‘å‘€ï¼æˆ‘è·‘å¾—å¯å¿«äº†ï¼(Catch me if you can!)'",
        "ðŸ˜„ ç¿»è¯‘: 'æˆ‘è¶…å¼€å¿ƒçš„ï¼æƒ³å’Œä½ ä¸€èµ·çŽ©ï¼(I'm super happy! Let's play!)'"
    ]
}

def translate_and_speak(audio_path):
    # è¯»å–éŸ³é¢‘
    audio_array, sr = librosa.load(audio_path, sr=16000)

    # é¢„æµ‹
    predictions = classifier(audio_array)
    top_prediction = predictions[0]
    label = top_prediction['label']
    score = top_prediction['score']

    # è½¬æ¢ä¸ºå°å†™ï¼Œé˜²æ­¢å¤§å°å†™ä¸åŒ¹é…
    label_key = label.lower()

    # æŸ¥å­—å…¸
    # å¦‚æžœå­—å…¸é‡Œæœ‰ï¼Œéšæœºé€‰ä¸€å¥ï¼›å¦‚æžœæ²¡æœ‰ï¼Œæ˜¾ç¤ºé»˜è®¤æç¤º
    texts = translation_script.get(label_key, [f"ðŸ¤” ç¿»è¯‘: æˆ‘å¬åˆ° '{label}' çš„å£°éŸ³ï¼Œä½†æ²¡æœ‰ç‰¹å®šçš„ç¿»è¯‘ã€‚(ç½®ä¿¡åº¦: {score:.2%})"])
    result_text = random.choice(texts)

    # è¯­éŸ³åˆæˆ
    # ä½¿ç”¨å®Œæ•´çš„ result_text è¿›è¡Œè¯­éŸ³åˆæˆï¼Œä¸å†æˆªæ–­ã€‚
    text_to_read = result_text
    # ç§»é™¤ç¿»è¯‘å†…å®¹ä¸­çš„è¡¨æƒ…ç¬¦å·ï¼Œé¿å… TTS æŠ¥é”™æˆ–å‘éŸ³å¥‡æ€ª
    text_to_read = text_to_read.split("ç¿»è¯‘:")[-1].strip() # æå–çº¯æ–‡æœ¬éƒ¨åˆ†
    text_to_read = ''.join(c for c in text_to_read if c.isalnum() or c.isspace() or c in '!.?,') # è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦

    tts_output = tts(text_to_read)

    return label, score, result_text, tts_output

# --- äº¤äº’ç•Œé¢ ---
print("\n" + "="*50)
print("ðŸŽ¤ ç‹—ç‹—æƒ…ç»ªç¿»è¯‘å®˜å·²ä¿®å¤ï¼çŽ°åœ¨èƒ½è¯†åˆ«ã€ç‹—ç‹—æƒ…ç»ªã€‘å¹¶è¿›è¡Œç¿»è¯‘äº†ã€‚") # ä¿®æ­£ç”¨æˆ·æç¤º
print("â¬‡ï¸ è¯·ä¸Šä¼ ä¸€ä¸ªç‹—ç‹—çš„å«å£°æ–‡ä»¶è¿›è¡Œæµ‹è¯•")
print("="*50)

uploaded = files.upload()

for filename in uploaded.keys():
    print(f"\nðŸ” åˆ†æžä¸­: {filename} ...")
    try:
        emotion, conf, text, speech = translate_and_speak(filename)

        print("-" * 30)
        print(f"ðŸ¶ è¯†åˆ«æƒ…ç»ª: ã€{emotion.upper()}ã€‘") # æ‰“å°å‡ºçœŸæ­£è¯†åˆ«åˆ°çš„è‹±æ–‡æ ‡ç­¾
        print(f"ðŸ“Š ç½®ä¿¡åº¦:   {conf:.2%}")
        print(f"ðŸ“ ç¿»è¯‘å†…å®¹: {text}")
        print("-" * 30)

        display(Audio(data=speech['audio'], rate=speech['sampling_rate']))

    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")
        # Cell 5.5: è¯„ä¼°æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨çŽ° (åˆ†ç±»æŠ¥å‘Šä¸Žæ··æ·†çŸ©é˜µ)
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("â³ æ­£åœ¨è¯„ä¼°æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨çŽ°...")

# èŽ·å–æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æžœ
predictions = trainer.predict(encoded_dataset["test"])
y_pred = np.argmax(predictions.predictions, axis=1) # é¢„æµ‹çš„ç±»åˆ«ID
y_true = predictions.label_ids # çœŸå®žçš„ç±»åˆ«ID

# å°†æ•°å­—æ ‡ç­¾IDè½¬æ¢å›žå­—ç¬¦ä¸²æ ‡ç­¾åç§°ï¼Œä»¥ä¾¿æŠ¥å‘Šæ›´æ˜“è¯»
# id2label æ˜¯ä»Ž Cell 2 ä¸­èŽ·å¾—çš„æ˜ å°„
y_pred_names = [id2label[str(label_id)] for label_id in y_pred]
y_true_names = [id2label[str(label_id)] for label_id in y_true]

# èŽ·å–æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾åç§°ï¼Œå¹¶ç¡®ä¿é¡ºåºä¸Ž id2label å¯¹åº”
target_names = [id2label[str(i)] for i in sorted([int(k) for k in id2label.keys()])]

# æ‰“å°åˆ†ç±»æŠ¥å‘Š
print("\n--- åˆ†ç±»æŠ¥å‘Š (Classification Report) ---")
print(classification_report(y_true_names, y_pred_names, target_names=target_names))

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
print("\n--- æ··æ·†çŸ©é˜µ (Confusion Matrix) ---")
cm = confusion_matrix(y_true_names, y_pred_names, labels=target_names)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)

fig, ax = plt.subplots(figsize=(8, 6))
disp.plot(cmap=plt.cm.Blues, ax=ax)
plt.title('Confusion Matrix for Dog Emotion Classification on Test Set')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print("âœ… æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨çŽ°è¯„ä¼°å®Œæˆï¼")
